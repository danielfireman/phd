# Methodology {#sec:met} 

On one hand performance has a direct and important impact on business objectives.
On the other hand, every service has a unique set of requirements and often
execute in resources which vary in vastly different ways. As an attempt to
provide adequate performance out of the box for the broadest range of
application-execution resource pairs, JVM (Java$\textsuperscript{TM}$ Virtual
Machine) comes  with a set of default parameters.

However, because no two applications are alike or use the runtime in exactly the
same fashion, there is no guarantee that any single set of parameters will be
perfectly suited for every pair. This reality highlights how important is to
conduct focused performance evaluation at runtime level. Thus, our goal with this
paper is to shed some light on this issue by assessing how the overhead that is
not application-specific (i.e. HTTP request handling framework, JVM maintenance 
tasks) impacts an asynchronous event-driven HTTP server that runs under high utilization.

In other words, our main goal to find which configuration leads to the greatest
saturation point[^1]. To do so, clients [^2] collect one piece of data: the number of requests processed per second (throughput, $T$). Based on this information, we can calculate an indirect metric:

[^1]: The saturation point tells that the service has reached its maximum capacity for the specific version (code), configuration and environment [@perfTestingWebSphere].

[^2]: Code can be found at https://git.io/v6lCf.

$$avgT(n_{vCPUs}) = \frac{T}{n_{vCPUs}}$$

Where $avgT(n_{VCPUs})$ is the average throughput per available virtual CPU (VCPU), $n_{VCPUs}$ is the number of vCPUs available for execution and $T$ is the throughput. Based on the latter, we could to formally express the experiment hypotheses:

* $H_{0}$: The maximum throughput increases linearly with the increasing number of vCPUs allocated to run a highly-loaded single-endpoint REST service on an asynchronous event-driven Java™ HTTP server configured with default JVM flags. Or more formally:

$$max(avgT(k)) \approx max(avgT(k+1)),\, \forall k > 0$$

* $H_{1}$: The maximum throughput does not increase linearly with the increasing number of vCPUs 
allocated to run a highly-loaded single-endpoint REST service on an asynchronous event-driven 
Java $\textsuperscript{TM}$ HTTP server configured with default JVM flags. Or more formally:

$$\exists k>0, \, max(avgT(k))\,\ne\, max(avgT(k+1))$$

## Experimental Setup

To achieve the experiment goal, the Java HTTP server business logic overhead was trimmed down.
The request handling internals was handled by i) Netty [@Maurer:2015:NA:2838857], an
asynchronous event-driven network application framework and ii) Jooby [@jooby],
a micro web framework for Java, both without any custom parameters or
configuration. The request handling logic was negligible, as well as the request
and response payloads[^3]. The Java HotSpot Virtual Machine was used in all
experiments.

[^3]: Code can be found at https://git.io/v64D7.

The load was impressed on the server using the monotonic step-function[@stepFun]
described bellow. At every 10 seconds, the load was increased by:

$$load(i) = i \cdot n_{vCPUs} \cdot 50, \, \forall x,y,\,\, x \leq y \implies load(x) \leq load(y)$$

Where $i$ is the index of the current step. At the beginning of every experiment the server received a load of $n_{vCPUs} * 50$ QPS. This warm up phase aimed to
mitigate the impact of Java's Just In Time (JIT) compiler and other internal JVM
mechanisms.

We conducted a series of experiments varying the number of CPUs available
at the virtual machine executing the server. Experiments were conducted using 1,
2 and 4 vCPUs (named *Exp1*, *Exp2*, *Exp4*). Each experiment was repeated 10
times. Table \ref{tab:expsetup} presents details of each experiment
configuration.

\begin{table}[H]
\centering
\begin{tabular}{lcccc}
  Name & \#vCPUs & RAM(GB) & Disk(GB) & \#Clients \\ 
  \hline\hline
  Exp1 & 1 & 0.50 & 10.00 & 1 \rule{0pt}{3ex} \\ 
  Exp2 & 2 & 4.00 & 60.00 & 3 \\ 
  Exp4 & 4 & 8.00 & 90.00 & 6 \\ 
\end{tabular}
\caption{Experiment setup.} 
\label{tab:expsetup}
\end{table}

As there is a negligible amount of serial work done at request handling (business
logic), one trying to predict the server performance based on either 
Amdahl’s Law (using $\sigma \approx 0$), Gustafson’s Law (using $\sigma'(1) \approx 0$)
or Super-Serial Model (using $\sigma \approx 0$ and $\gamma \approx 0$) would
expect a speedup very close to linear[@DBLP:conf/cmg/WilliamsS04].
In other words, the degree of parallelism in the application is such that it could
make (almost) full use of the additional resources provided by scaling.

To our surprise, experiment results refute that hypothesis. These results are
presented at next Section.

<!--Each client output the number of requests successfully attended by the
server per second during each step (throughput). The data was collected and processed to generate consolidated view of the troughput[^2].

[^2]: Code can be found at https://git.io/v6lCf.

The consolidated view has one entry per step, each entry contains:

* Load: number of requests per second sent to the server during the step
* Throughput: number requests successfully processed by the server per second
during the step.

Lastly the column $avgt$ is added. This column represents $avgT(n_{vCPUs})$ and it is calculated by dividing the throughput of each row by the number of available vCPUs available at the VM used to the run the HTTP server. The code used to calculate this and all other indirect metrics can be found [here](https://github.com./danielfireman/phd). -->
